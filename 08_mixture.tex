\documentclass[aspectratio=169,unicode,dvipdfmx,14pt]{beamer}

\input{mycmds.tex}

\title{ \\混合分布}
\author{\texorpdfstring{正田 備也\newline\href{mailto:masada@rikkyo.ac.jp}{masada@rikkyo.ac.jp}}{正田 備也}}
\date{}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\section{なぜ混合分布を使うのか}

\begin{frame}\frametitle{Contents}
\Large \tableofcontents[currentsection]
\end{frame}

\begin{frame}{これまでのモデリングの問題点}
\begin{itemize}
\item これまでは、データ集合$\mathcal{D}=\{\bm{x}_1, \ldots, \bm{x}_N\}$全体に対して、
一つの確率分布を使うモデリングだけ議論していた
\item しかし、多くのデータ集合は、たった一つの分布ではモデリングし切れない多様性を含んでいる
\item 例えば、数値データの集合であれば、その周辺の数値が頻繁に出現するという数値が、複数あったりする
\begin{itemize}
\item 例：多峰性をもつデータ集合
\end{itemize}
\end{itemize}
\begin{textblock*}{0.4\linewidth}(320pt, 175pt)
    \centering
    \includegraphics[width=0.7\linewidth]{Bimodal-bivariate-small.png}
\end{textblock*}

\end{frame}

\begin{frame}{混合分布}
\begin{itemize}
\item これまでは、全てのデータ$\bm{x}_i$ for $i=1,\ldots,N$を、
同じ一つの分布からdrawしていた
\begin{itemize}
\item 全ての確率変数$\bm{x}_i$ for $i=1,\ldots,N$が同じ分布に従うと考えていた
\end{itemize}
\item 一方、混合分布によるモデリングでは、同じ種類の分布だがパラメータの値が違うだけの分布を、$K$個用意する
\begin{itemize}
\item これらの分布をコンポーネントと呼ぶ
\end{itemize}
\item そして、各データ$\bm{x}_i$について、まず、カテゴリカル分布$\mbox{Cat}(\bm{\theta})$にしたがって、
$K$個のコンポーネントから一つ選ぶ
\begin{itemize}
\item $\theta_k$は$k$番目のコンポーネントが選ばれる確率
\begin{itemize}
\item もちろん$\sum_k \theta_k = 1$が成り立つ
\end{itemize}
\end{itemize}
\item そして、$\bm{x}_i$がその選ばれた分布に従うと考える。
\end{itemize}
\end{frame}


\section{混合正規分布}

\begin{frame}\frametitle{Contents}
\Large \tableofcontents[currentsection]
\end{frame}

\begin{frame}{混合正規分布}
\begin{itemize}
\item 混合正規分布を使ったモデリングでは、データ集合$\mathcal{D}=\{\bm{x}_1, \ldots, \bm{x}_N\}$が以下のように生成されると仮定する
\item $i$番目のデータ$\bm{x}_i$を生成するために、まず、カテゴリカル分布$\mbox{Cat}(\bm{\theta})$から、
確率変数$z_i$の値をdrawする
\begin{itemize}
\item $z_i = k$は、$k$番目のコンポーネントが選ばれたことを意味する
\end{itemize}
\item その$z_i$の値に対応する確率分布から、$\bm{x}_i$をdrawする
\begin{align}
z_i & \sim \mbox{Cat}(\bm{\theta}) \notag \\
\bm{x}_i & \sim \mathcal{N}(\bm{\mu}_{z_i}, \bm{\Sigma}_{z_i})
\end{align}
\end{itemize}
\end{frame}

\begin{frame}{単変量正規分布の混合分布の場合}
\begin{itemize}
\item $K$個のコンポーネントからひとつを選ぶカテゴリカル分布のパラメータは
$\bm{\theta} = (\theta_1,\ldots,\theta_K)$
\begin{itemize}
\item $\theta_k$は$k$番目のコンポーネントが選ばれる確率
\item $\sum_{k=1}^K \theta_k = 1$が成り立つ
\end{itemize}
\item どのコンポーネントも単変量正規分布で、$k$番目のコンポーネントのパラメータは平均$\mu_k$と標準偏差$\sigma_k$
\begin{itemize}
\item その確率密度関数は
\end{itemize}
\begin{align}
p(x;\mu_k,\sigma_k) = \frac{1}{\sqrt{2\pi\sigma_k^2}}\exp\Big( - \frac{(x - \mu_k)^2}{2\sigma_k^2} \Big)
\end{align}
\end{itemize}
\end{frame}

\begin{frame}{観測データの尤度}
\begin{itemize}
\item 単変量正規分布の混合分布でモデリングされた観測データの尤度は
\vspace{-.1in}
\begin{align}
p(\mathcal{D};\bm{\theta},\bm{\mu},\bm{\sigma})
& = \prod_{i=1}^N p(x_i ; \theta_{z_i}, \sigma_{z_i}) \notag \\ &
= \prod_{i=1}^N \bigg[ \theta_{z_i} \times \frac{1}{\sqrt{2\pi\sigma_{z_i}^2}}\exp\bigg( - \frac{(x_i - \mu_{z_i})}{2\sigma_{z_i}^2} \bigg) \bigg]
\end{align}
\vspace{-.1in}
\begin{itemize}
\item 個々のデータは、同じ分布からではないにせよ、独立に生成されると仮定している
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{教師ありの設定の場合}
\begin{itemize}
\item 教師ありの設定の場合、各データ$x_i$について、それがどのコンポーネントから生成されたかは、すでに分かっている
\item 言い換えれば、$z_i$の値も観測データに含まれる
\begin{itemize}
\item つまり、$\mathcal{D} = \{ (x_1,z_1), \ldots, (x_N,z_N) \}$
\end{itemize}
\item このとき、観測データ$\mathcal{D}$の尤度は
\vspace{-.1in}
\begin{align}
& p(\mathcal{D};\bm{\theta},\mu_1,\ldots,\mu_K,\sigma_1,\ldots,\sigma_K)
\notag \\ &
= \prod_{k=1}^K \bigg[ \theta_k^{c_k} \times \frac{1}{ (\sqrt{2\pi\sigma_k^2} )^{c_k}} 
\exp\bigg( - \frac{ \sum_{ \{ i : z_i = k \} } (x_i - \mu_k)^2 }{2\sigma_k^2} \bigg) \bigg]
\end{align}
\vspace{-.1in}
\begin{itemize}
\item $c_k$は、$k$番目のコンポーネントから生成されたデータの個数
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\FontMath
\begin{align}
& L(\bm{\theta},\mu_1,\ldots,\mu_K,\sigma_1,\ldots,\sigma_K)
= \ln p(\mathcal{D};\bm{\theta},\mu_1,\ldots,\mu_K,\sigma_1,\ldots,\sigma_K) 
+ \lambda \bigg( 1 - \sum_{k=1}^K \theta_k \bigg)
\notag \\ &
= \sum_{k=1}^K c_k \ln \theta_k - \sum_{k=1}^K c_k \ln \sigma_k
- \sum_{k=1}^K \frac{\sum_{ \{ i : z_i = k \} } (x_i - \mu_k)^2}{ 2\sigma_k^2} + \lambda \bigg( 1 - \sum_{k=1}^K \theta_k \bigg) + const.
\end{align}
\begin{align}
\frac{\partial L}{\partial \mu_k} = \frac{ \sum_{ \{ i : z_i = k \} } (x_i - \mu_k) }{\sigma_k^2}
= \frac{ \sum_{ \{ i : z_i = k \} } x_i - c_k \mu_k }{\sigma_k^2}
\end{align}
$\frac{\partial L}{\partial \mu_k} = 0$より、$\mu_k = \frac{ \sum_{ \{ i : z_i = k \} } x_i }{ c_k } = \bar{x}_k$を得る。
\begin{align}
\frac{\partial L}{\partial \sigma_k} = - \frac{c_k}{\sigma_k} + \frac{\sum_{ \{ i : z_i = k \} } (x_i - \mu_k)^2}{ \sigma_k^3}
\end{align}
$\frac{\partial L}{\partial \sigma_k} = 0$より、
$\sigma_k^2 = \frac{\sum_{ \{ i : z_i = k \} } (x_i - \bar{x}_k)^2}{ \sigma_k^2}$を得る。
\end{frame}

\begin{frame}
\FontMath
\begin{align}
\frac{\partial L}{\partial \theta_k} = \frac{c_k}{\theta_k} - \lambda \mbox{ , \ }
\frac{\partial L}{\partial \lambda} = 1 - \sum_{k=1}^K \theta_k
\end{align}
$\frac{\partial L}{\partial \theta_k} = 0$より、$\theta_k = \frac{c_k}{\lambda}$を得る。

$\frac{\partial L}{\partial \lambda} = 0$より、$1 - \sum_{k=1}^K \frac{c_k}{\lambda} = 0$を得る。

つまり、$\lambda = \sum_k c_k$が言えるので、$\theta_k = \frac{c_k}{\sum_k c_k}$を得る。

\

まとめると、
\begin{itemize}
\item $\theta_k$は、$k$番目のコンポーネントから生成されたデータの割合となる。
\item $\mu_k$と$\sigma_k$は、$k$番目のコンポーネントから生成されたデータだけの尤度をもとに最尤推定した値となる。
\end{itemize}
\end{frame}

\begin{frame}{教師なしの設定の場合}
\begin{itemize}
\item 教師なしの設定の場合、各データ$x_i$について、それがどのコンポーネントから生成されたかは、分からない！
\item $z_i$は、値が観測されない確率変数、すなわち潜在変数
\begin{itemize}
\item つまり、$\mathcal{D} = \{ x_1, \ldots, x_N \}$
\item 一方、潜在変数の集合を$\mathcal{Z} = \{ z_1, \ldots, z_N \}$とする
\end{itemize}
\item このとき、観測データ$\mathcal{D}$の尤度は、どう書けばいいのか？
\begin{itemize}
\item 下の式で与えられる$p(\mathcal{D}, \mathcal{Z})$は、観測データの尤度$p(\mathcal{D})$ではない
\end{itemize}
\vspace{-.1in}
\begin{align}
& p(\mathcal{D}, \mathcal{Z};\bm{\theta},\mu_1,\ldots,\mu_K,\sigma_1,\ldots,\sigma_K)
\notag \\ &
= \prod_{k=1}^K \bigg[ \theta_k^{c_k} \times \frac{1}{ (\sqrt{2\pi\sigma_k^2} )^{c_k}} 
\exp\bigg( - \frac{ \sum_{ \{ i : z_i = k \} } (x_i - \mu_k)^2 }{2\sigma_k^2} \bigg) \bigg]
\end{align}
\end{itemize}
\end{frame}

\begin{frame}{周辺尤度}
\begin{itemize}
\item 潜在変数を含むモデリングの場合、観測データの尤度$p(\mathcal{D})$は、
潜在変数を周辺化marginalizeしてはじめて得られる
\begin{itemize}
\item 周辺化によって得られる尤度を周辺尤度marginal likelihoodと呼ぶ
\end{itemize}
\begin{align}
& p(\mathcal{D}) = \sum_{\mathcal{Z}} p(\mathcal{D}, \mathcal{Z})
\notag \\ &
= \sum_{z_1=1}^K \sum_{z_2=1}^K \cdots \sum_{z_{N-1}=1}^K \sum_{z_N=1}^K p(\mathcal{D}, \mathcal{Z})
\end{align}
\item 上の式で足し合わされている項は、$K^N$個もあって、妥当な時間内では計算できない！
\begin{itemize}
\item いわゆる、組合せ論的爆発combinatorial explosion
\end{itemize}
\end{itemize}
\end{frame}



\end{document}